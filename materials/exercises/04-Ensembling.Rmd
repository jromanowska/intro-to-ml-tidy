---
title: "04-Ensembling"
output: html_document
---

```{r setup}
library(tidyverse)
library(tidymodels)
library(tune)

source(here::here("materials/exercises/04-helpers.R"))

fit_split <- function(formula, model, split, ...) {
  wf <- workflows::add_model(workflows::add_formula(workflows::workflow(), formula, blueprint = hardhat::default_formula_blueprint(indicators = "none")), model)
  tune::last_fit(wf, split, ...)
}

get_tree_fit <- function(results) {
  results %>% 
    pluck(".workflow", 1) %>% 
    workflows::pull_workflow_fit() 
}

# read in the data
stackoverflow <- read_rds(here::here("materials/data/stackoverflow.rds"))

# split the data
set.seed(100) # Important!
so_split <- initial_split(stackoverflow, strata = remote)
so_train <- training(so_split)
so_test  <- testing(so_split)
```

# Your turn 1

Fill in the blanks to return the accuracy and ROC AUC for the vanilla decision tree model.

```{r}
vanilla_tree_spec <-
  decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

set.seed(100) # Important!
fit_split(remote ~ ., 
          model = vanilla_tree_spec, 
          split = so_split) %>% 
  collect_metrics()

args(vanilla_tree_spec)
```


# Your Turn 2

Create a new classification tree model spec; call it `big_tree_spec`. 
Set the cost complexity to `0`, and the minimum number of data points in a node to split to be `1`. 

Compare the metrics of the big tree to the vanilla tree- which one predicts the test set better?

*Hint: you'll need https://tidymodels.github.io/parsnip/reference/decision_tree.html*

```{r}
big_tree_spec <-
  decision_tree(cost_complexity = 0., min_n = 1) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

fit_split(remote ~ ., 
          model = big_tree_spec, 
          split = so_split) %>% 
  collect_metrics()
```


# Your Turn 3

Let's combine bootstrapping with decision trees.

Do **Round 1** on your handouts.

# Your Turn 4

Now, let's add the aggregating part.

Do **Round 2** on your handouts.

# Your Turn 5

Create a new model spec called `rf_spec`, which will learn an ensemble of classification trees from our training data using the **ranger** package. 

Compare the metrics of the random forest to your two single tree models (vanilla and big)- which predicts the test set better?

*Hint: you'll need https://tidymodels.github.io/parsnip/articles/articles/Models.html*

```{r}
set.seed(100) # Important!
rf_spec <-
  rand_forest(mode = "classification") %>%
  set_engine("ranger")

fit_split(remote ~ .,
          model = rf_spec,
          split = so_split) %>%
  collect_metrics()

fit_split(remote ~ .,
          model = rf_spec,
          split = so_split) %>%
  collect_predictions() %>%
  roc_curve(truth = remote, estimate = .pred_Remote) %>%
  autoplot()
```

# Your Turn 6

Challenge: Make 4 more random forest model specs, each using 4, 8, 12, and 20 variables at each split. Which value maximizes the area under the ROC curve?

*Hint: you'll need https://tidymodels.github.io/parsnip/reference/rand_forest.html*

```{r}
mtry_vect <- c(4, 8, 12, 20)
metrics <- purrr::map(mtry_vect, function(mtry_val){
  rf_spec <-
    rand_forest(mtry = mtry_val) %>%
    set_engine("ranger") %>%
    set_mode("classification")
  
  set.seed(100)
  fit_split(remote ~ .,
            model = rf_spec,
            split = so_split) %>%
    collect_metrics() %>%
    mutate(cur_mtry = mtry_val)
})
metrics_all <- bind_rows(metrics)
metrics_all
```

```{r}
ggplot(metrics_all, aes(cur_mtry, .estimate)) +
  geom_line(aes(group = .metric, color = .metric)) +
  xlab("mtry") + ylab("estimate")
```

# Bagging

```{r}
treebag_spec <-
  rand_forest(mtry = .preds()) %>%
  set_engine("ranger") %>% 
  set_mode("classification")

set.seed(100)
fit_split(remote ~ ., 
          model = treebag_spec,
          split = so_split) %>% 
  collect_metrics()
```

# Fitting based on importance of variables

```{r}
rf_imp_spec <-
  rand_forest(mtry = 4) %>% 
  set_engine("ranger", importance = 'impurity') %>% 
  set_mode("classification")

imp_fit <- 
  fit_split(remote ~ ., 
            model = rf_imp_spec,
            split = so_split)

imp_fit

source("04-helpers.R")
get_tree_fit(imp_fit)
```

A package to visualize importance of variables: https://koalaverse.github.io/vip/index.html

```{r}
imp_plot <- get_tree_fit(imp_fit)
vip::vip(imp_plot, geom = "point")
```


# Your Turn 7

Make a new model spec called `treebag_imp_spec` to fit a bagged classification tree model. Set the variable `importance` mode to "permutation". Plot the variable importance- which variable was the most important?

```{r}
treebag_imp_spec <- rand_forest(mtry = .preds()) %>%
  set_engine("ranger", importance = "permutation") %>% 
  set_mode("classification")

set.seed(100)
treebag_imp_fit <- fit_split(remote ~ ., 
          model = treebag_imp_spec,
          split = so_split)
treebag_imp_plot <- get_tree_fit(treebag_imp_fit)
vip::vip(treebag_imp_plot, geom = "col")
```




